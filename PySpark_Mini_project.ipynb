{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PySpark Mini-project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Storm events in US during 2019 are studied in the following analysis using the NOAA's National Weather Service Dataset available [here](https://www.ncdc.noaa.gov/stormevents/ftp.jsp). The folder **data** contains three csv files with event details, fatalities and location of every event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName(\"pysparkDataframes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStorm2019 = sc.read.format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .option('delimiter', ',')\\\n",
    "                .option('inferSchema', 'true')\\\n",
    "                .load('../pyspark/data/StormEvents_details-ftp_v1.0_d2019_c20200317.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFatalities2019 = sc.read.format('csv')\\\n",
    "                    .option('header', 'true')\\\n",
    "                    .option('delimiter', ',')\\\n",
    "                    .option('inferSchema', 'true')\\\n",
    "                    .load('../pyspark/data/StormEvents_fatalities-ftp_v1.0_d2019_c20200317.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLocations2019 = sc.read.format('csv')\\\n",
    "                    .option('header', 'true')\\\n",
    "                    .option('delimiter', ',')\\\n",
    "                    .option('inferSchema', 'true')\\\n",
    "                    .load('../pyspark/data/StormEvents_locations-ftp_v1.0_d2019_c20200317.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the schema of every dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BEGIN_YEARMONTH: integer (nullable = true)\n",
      " |-- BEGIN_DAY: integer (nullable = true)\n",
      " |-- BEGIN_TIME: integer (nullable = true)\n",
      " |-- END_YEARMONTH: integer (nullable = true)\n",
      " |-- END_DAY: integer (nullable = true)\n",
      " |-- END_TIME: integer (nullable = true)\n",
      " |-- EPISODE_ID: integer (nullable = true)\n",
      " |-- EVENT_ID: integer (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- STATE_FIPS: integer (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH_NAME: string (nullable = true)\n",
      " |-- EVENT_TYPE: string (nullable = true)\n",
      " |-- CZ_TYPE: string (nullable = true)\n",
      " |-- CZ_FIPS: integer (nullable = true)\n",
      " |-- CZ_NAME: string (nullable = true)\n",
      " |-- WFO: string (nullable = true)\n",
      " |-- BEGIN_DATE_TIME: string (nullable = true)\n",
      " |-- CZ_TIMEZONE: string (nullable = true)\n",
      " |-- END_DATE_TIME: string (nullable = true)\n",
      " |-- INJURIES_DIRECT: integer (nullable = true)\n",
      " |-- INJURIES_INDIRECT: integer (nullable = true)\n",
      " |-- DEATHS_DIRECT: integer (nullable = true)\n",
      " |-- DEATHS_INDIRECT: integer (nullable = true)\n",
      " |-- DAMAGE_PROPERTY: string (nullable = true)\n",
      " |-- DAMAGE_CROPS: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- MAGNITUDE: double (nullable = true)\n",
      " |-- MAGNITUDE_TYPE: string (nullable = true)\n",
      " |-- FLOOD_CAUSE: string (nullable = true)\n",
      " |-- CATEGORY: integer (nullable = true)\n",
      " |-- TOR_F_SCALE: string (nullable = true)\n",
      " |-- TOR_LENGTH: double (nullable = true)\n",
      " |-- TOR_WIDTH: double (nullable = true)\n",
      " |-- TOR_OTHER_WFO: string (nullable = true)\n",
      " |-- TOR_OTHER_CZ_STATE: string (nullable = true)\n",
      " |-- TOR_OTHER_CZ_FIPS: integer (nullable = true)\n",
      " |-- TOR_OTHER_CZ_NAME: string (nullable = true)\n",
      " |-- BEGIN_RANGE: integer (nullable = true)\n",
      " |-- BEGIN_AZIMUTH: string (nullable = true)\n",
      " |-- BEGIN_LOCATION: string (nullable = true)\n",
      " |-- END_RANGE: integer (nullable = true)\n",
      " |-- END_AZIMUTH: string (nullable = true)\n",
      " |-- END_LOCATION: string (nullable = true)\n",
      " |-- BEGIN_LAT: double (nullable = true)\n",
      " |-- BEGIN_LON: double (nullable = true)\n",
      " |-- END_LAT: double (nullable = true)\n",
      " |-- END_LON: double (nullable = true)\n",
      " |-- EPISODE_NARRATIVE: string (nullable = true)\n",
      " |-- EVENT_NARRATIVE: string (nullable = true)\n",
      " |-- DATA_SOURCE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FAT_YEARMONTH: integer (nullable = true)\n",
      " |-- FAT_DAY: integer (nullable = true)\n",
      " |-- FAT_TIME: integer (nullable = true)\n",
      " |-- FATALITY_ID: integer (nullable = true)\n",
      " |-- EVENT_ID: integer (nullable = true)\n",
      " |-- FATALITY_TYPE: string (nullable = true)\n",
      " |-- FATALITY_DATE: string (nullable = true)\n",
      " |-- FATALITY_AGE: integer (nullable = true)\n",
      " |-- FATALITY_SEX: string (nullable = true)\n",
      " |-- FATALITY_LOCATION: string (nullable = true)\n",
      " |-- EVENT_YEARMONTH: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFatalities2019.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEARMONTH: integer (nullable = true)\n",
      " |-- EPISODE_ID: integer (nullable = true)\n",
      " |-- EVENT_ID: integer (nullable = true)\n",
      " |-- LOCATION_INDEX: integer (nullable = true)\n",
      " |-- RANGE: double (nullable = true)\n",
      " |-- AZIMUTH: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- LAT2: integer (nullable = true)\n",
      " |-- LON2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfLocations2019.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes have a **registerTempTable attribute** that can be transform into spark sql to generate queries and save the results using **.write.saveAsTable(name_table)**. RDD's don't have this extension because they are not structured tables. \n",
    "\n",
    "Obs: **registerTempTable** is deprecated. Instead, use **createOrReplaceTempView**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStorm2019.createOrReplaceTempView('stormDetails_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number of events by STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|         STATE|NUMBER_EVENTS|\n",
      "+--------------+-------------+\n",
      "|         TEXAS|         4338|\n",
      "|        KANSAS|         2672|\n",
      "|    CALIFORNIA|         2643|\n",
      "|  SOUTH DAKOTA|         2543|\n",
      "|      NEW YORK|         2514|\n",
      "|      VIRGINIA|         2398|\n",
      "|  PENNSYLVANIA|         2395|\n",
      "|          OHIO|         2279|\n",
      "|          IOWA|         2276|\n",
      "|      MISSOURI|         2159|\n",
      "|     MINNESOTA|         2126|\n",
      "|      NEBRASKA|         2088|\n",
      "|      ILLINOIS|         2084|\n",
      "|      OKLAHOMA|         1801|\n",
      "|      COLORADO|         1776|\n",
      "|     WISCONSIN|         1573|\n",
      "|      KENTUCKY|         1522|\n",
      "|NORTH CAROLINA|         1448|\n",
      "|       INDIANA|         1447|\n",
      "|       MONTANA|         1286|\n",
      "+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT STATE, COUNT(STATE) AS NUMBER_EVENTS \\\n",
    "                FROM stormDetails_table \\\n",
    "                GROUP BY STATE ORDER BY COUNT(STATE) DESC').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Duration of events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we inspect the date information available in the stormDetails table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----------+-------------+-------+--------+\n",
      "|BEGIN_YEARMONTH|BEGIN_DAY|BEGIN_TIME|END_YEARMONTH|END_DAY|END_TIME|\n",
      "+---------------+---------+----------+-------------+-------+--------+\n",
      "|         201905|        9|      1554|       201905|      9|    1830|\n",
      "|         201907|       15|      1640|       201907|     15|    1641|\n",
      "|         201910|       20|      2223|       201910|     20|    2223|\n",
      "|         201910|       20|      2312|       201910|     20|    2312|\n",
      "|         201910|       20|      2236|       201910|     20|    2236|\n",
      "+---------------+---------+----------+-------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT BEGIN_YEARMONTH, BEGIN_DAY, BEGIN_TIME,  \\\n",
    "                END_YEARMONTH, END_DAY, END_TIME \\\n",
    "                FROM stormDetails_table').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      BEGIN_DATE|\n",
      "+----------------+\n",
      "| 2019-05-9 15:54|\n",
      "|2019-07-15 16:40|\n",
      "|2019-10-20 22:23|\n",
      "|2019-10-20 23:12|\n",
      "|2019-10-20 22:36|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT CONCAT(SUBSTRING(BEGIN_YEARMONTH, 1, 4), '-', \\\n",
    "                SUBSTRING(BEGIN_YEARMONTH, 5, 2), '-', \\\n",
    "                BEGIN_DAY, ' ', SUBSTRING(BEGIN_TIME, 1, 2), ':', SUBSTRING(BEGIN_TIME, 3, 2)) AS BEGIN_DATE \\\n",
    "                FROM stormDetails_table\").show(5)   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT DATEDIFF(month, '2017/08/25', '2011/08/25') AS DateDiff;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stormPandas = dfStorm2019.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
