{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark on Mac\n",
    "\n",
    "To install PySpark, I used this [tutorial](https://medium.com/@GalarnykMichael/install-spark-on-mac-pyspark-453f395f240b) that offers a complete guideline about how to configure and update PySpark driver environment variables adding lines to your ~/.bash_profile file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDDs are the backbone of Apache Spark. They perform calculations faster because the dataset is parallelized, it means, distributed or split into chuncks based on keys and executor nodes.  \n",
    "\n",
    "The transformations to the dataset only occur when the action is taken, optimizing the execution.\n",
    "\n",
    "Let's try an example of RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 million of 2D dots are randomly generated. A basic multiplication and substraction is applied to every coordinate and then we calculate the mean and standard deviation of every population of coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "TOTAL = 1000000\n",
    "dots = sc.parallelize([2.0 * np.random.random(2) - 1.0 for i in range(TOTAL)]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.55811266, 0.51695219]),\n",
       " array([0.60579333, 0.97973035]),\n",
       " array([-0.04970269, -0.04261188]),\n",
       " array([0.66438197, 0.8183018 ]),\n",
       " array([0.55038452, 0.12525442]),\n",
       " array([ 0.19769196, -0.13486087]),\n",
       " array([ 0.12383726, -0.88487532]),\n",
       " array([-0.78083423,  0.6238091 ]),\n",
       " array([-0.14873242,  0.35647251]),\n",
       " array([0.84752257, 0.52233492])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#access to the first 10 elements on dots:\n",
    "dots.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the elements on dots:\n",
    "dots.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55811266, 0.51695219])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect firt line\n",
    "dots.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [-0.00027926 -0.00098815]\n",
      "stdev: [0.57728909 0.57736694]\n"
     ]
    }
   ],
   "source": [
    "stats = dots.stats()\n",
    "print('Mean:', stats.mean())\n",
    "print('stdev:', stats.stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Data Transformations\n",
    "\n",
    "\n",
    "What kind of transformations we can do? Mapping, filtering, joining, and transcoding are the operations that transform the values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['a', 'b', 'c', 1, 1.1]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flat white', 2), ('latte', 1), ('pour over', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('flat white', 1), ('latte', 4), ('pour over', 1), ('flat white', 3)]) \n",
    "sorted(rdd.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.parallelize(['a', 'b', 'c', 'd', 'e', 'a', 'b']).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cup of flat white',\n",
       " 'cup of capuccino',\n",
       " 'cup of latte',\n",
       " 'cup of tea',\n",
       " 'cup of matcha']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['flat white', 'capuccino', 'latte', 'tea', 'matcha'])\n",
    "rdd.map(lambda x: 'cup of '+''.join(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flat white'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['flat white', 'capuccino', 'latte', 'tea', 'matcha'])\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Using PySpark to perform Transformations and Actions on RDD](https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/?utm_source=blog&utm_medium=DataFramePySparkarticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sp.textFile(\"../pyspark/data/blogtexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map and flatMap\n",
    "\n",
    "#### Q1: Convert all words in a rdd to lowercase and split the lines of a document using space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd.map(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['think',\n",
       "  'of',\n",
       "  'it',\n",
       "  'for',\n",
       "  'a',\n",
       "  'moment',\n",
       "  '–',\n",
       "  '1',\n",
       "  'qunitillion',\n",
       "  '=',\n",
       "  '1',\n",
       "  'million',\n",
       "  'billion!',\n",
       "  'can',\n",
       "  'you',\n",
       "  'imagine',\n",
       "  'how',\n",
       "  'many',\n",
       "  'drives',\n",
       "  '/',\n",
       "  'cds',\n",
       "  '/',\n",
       "  'blue-ray',\n",
       "  'dvds',\n",
       "  'would',\n",
       "  'be',\n",
       "  'required',\n",
       "  'to',\n",
       "  'store',\n",
       "  'them?',\n",
       "  'it',\n",
       "  'is',\n",
       "  'difficult',\n",
       "  'to',\n",
       "  'imagine',\n",
       "  'this',\n",
       "  'scale',\n",
       "  'of',\n",
       "  'data',\n",
       "  'generation',\n",
       "  'even',\n",
       "  'as',\n",
       "  'a',\n",
       "  'data',\n",
       "  'science',\n",
       "  'professional.',\n",
       "  'while',\n",
       "  'this',\n",
       "  'pace',\n",
       "  'of',\n",
       "  'data',\n",
       "  'generation',\n",
       "  'is',\n",
       "  'very',\n",
       "  'exciting,',\n",
       "  'it',\n",
       "  'has',\n",
       "  'created',\n",
       "  'entirely',\n",
       "  'new',\n",
       "  'set',\n",
       "  'of',\n",
       "  'challenges',\n",
       "  'and',\n",
       "  'has',\n",
       "  'forced',\n",
       "  'us',\n",
       "  'to',\n",
       "  'find',\n",
       "  'new',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'big',\n",
       "  'huge',\n",
       "  'data',\n",
       "  'effectively.'],\n",
       " []]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', 'of', 'it', 'for', 'a', 'moment', '–', '1', 'qunitillion', '=']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "#### Q2: Remove stopwords and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS.add('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: re.sub('[^A-Za-z0-9]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', 'of', 'it', 'for', 'a', 'moment', '', '1', 'qunitillion', '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.filter(lambda x: x not in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think',\n",
       " 'moment',\n",
       " '1',\n",
       " 'qunitillion',\n",
       " '1',\n",
       " 'million',\n",
       " 'billion',\n",
       " 'imagine',\n",
       " 'drives',\n",
       " 'cds']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "#### Q3: Group the words in rdd4 based on which letters they start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5 = rdd4.groupBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 227),\n",
       " ('m', 127),\n",
       " ('1', 23),\n",
       " ('q', 7),\n",
       " ('b', 63),\n",
       " ('i', 109),\n",
       " ('d', 233),\n",
       " ('c', 275),\n",
       " ('r', 187),\n",
       " ('s', 371),\n",
       " ('g', 20),\n",
       " ('p', 199),\n",
       " ('e', 88),\n",
       " ('n', 94),\n",
       " ('f', 118),\n",
       " ('w', 49),\n",
       " ('h', 63),\n",
       " ('u', 34),\n",
       " ('o', 67),\n",
       " ('a', 171)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, len(list(value))) for k, value in rdd5.take(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t',\n",
       "  ['think',\n",
       "   'typically',\n",
       "   'turns',\n",
       "   'terms',\n",
       "   'time',\n",
       "   'traditional',\n",
       "   'topic',\n",
       "   'table',\n",
       "   'terms',\n",
       "   'traditional',\n",
       "   'tough',\n",
       "   'task',\n",
       "   'temperature',\n",
       "   'time',\n",
       "   'typically',\n",
       "   'task',\n",
       "   'task',\n",
       "   'time',\n",
       "   'time',\n",
       "   'task',\n",
       "   'task',\n",
       "   'terms',\n",
       "   'time',\n",
       "   'task',\n",
       "   'task',\n",
       "   'time',\n",
       "   'tasks',\n",
       "   'times',\n",
       "   'times',\n",
       "   'traditional',\n",
       "   'tasks',\n",
       "   'terms',\n",
       "   'types',\n",
       "   'terms',\n",
       "   'tasks',\n",
       "   'traditional',\n",
       "   'talk',\n",
       "   'times',\n",
       "   'tasks',\n",
       "   'turn',\n",
       "   'terms',\n",
       "   'time',\n",
       "   'time',\n",
       "   'tasks',\n",
       "   'technologies',\n",
       "   'time',\n",
       "   'task',\n",
       "   'time',\n",
       "   'transactions',\n",
       "   'time',\n",
       "   'time',\n",
       "   'think',\n",
       "   'transformations',\n",
       "   'transformations',\n",
       "   'topic',\n",
       "   'talking',\n",
       "   'terminal',\n",
       "   'terms',\n",
       "   'type',\n",
       "   'transfers',\n",
       "   'tar',\n",
       "   'tar',\n",
       "   'tool',\n",
       "   'time',\n",
       "   'typing',\n",
       "   'typing',\n",
       "   'typing',\n",
       "   'typing',\n",
       "   'thread',\n",
       "   'time',\n",
       "   'time',\n",
       "   'transformations',\n",
       "   'talk',\n",
       "   'tolerant',\n",
       "   'transformations',\n",
       "   'things',\n",
       "   'types',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformations',\n",
       "   'task',\n",
       "   'task',\n",
       "   'types',\n",
       "   'type',\n",
       "   'transformation',\n",
       "   'tells',\n",
       "   'tuple',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'transformed',\n",
       "   'transformation',\n",
       "   'transformation',\n",
       "   'time',\n",
       "   'transformation',\n",
       "   'talk',\n",
       "   'table',\n",
       "   'tutorial',\n",
       "   'table',\n",
       "   'type',\n",
       "   'train',\n",
       "   'test',\n",
       "   'train',\n",
       "   'trueinferschema',\n",
       "   'true',\n",
       "   'test',\n",
       "   'trueinferschema',\n",
       "   'true',\n",
       "   'train',\n",
       "   'test',\n",
       "   'true',\n",
       "   'true',\n",
       "   'telling',\n",
       "   'type',\n",
       "   'true',\n",
       "   'type',\n",
       "   'types',\n",
       "   'train',\n",
       "   'tree',\n",
       "   'trainprintschema',\n",
       "   'train',\n",
       "   'trainhead10',\n",
       "   'train',\n",
       "   'traincount',\n",
       "   'train',\n",
       "   'test',\n",
       "   'trainnadropcounttestnadropanycount',\n",
       "   'train',\n",
       "   'test',\n",
       "   'techniques',\n",
       "   'transform',\n",
       "   'train',\n",
       "   'test',\n",
       "   'train',\n",
       "   'trainfillna1',\n",
       "   'test',\n",
       "   'testfillna1',\n",
       "   'traindescribeshow',\n",
       "   'train',\n",
       "   'trainselectuseridshow',\n",
       "   'train',\n",
       "   'test',\n",
       "   'train',\n",
       "   'test',\n",
       "   'trainselectproductiddistinctcount',\n",
       "   'testselectproductiddistinctcount',\n",
       "   'train',\n",
       "   'test',\n",
       "   'train',\n",
       "   'test',\n",
       "   'test',\n",
       "   'train',\n",
       "   'test',\n",
       "   'train',\n",
       "   'test',\n",
       "   'train',\n",
       "   'transforming',\n",
       "   'transform',\n",
       "   'transformation',\n",
       "   'train',\n",
       "   'transform',\n",
       "   'train',\n",
       "   'test',\n",
       "   'transform',\n",
       "   'train',\n",
       "   'test',\n",
       "   'transform',\n",
       "   'transformation',\n",
       "   'train1',\n",
       "   'test1',\n",
       "   'train1',\n",
       "   'test1',\n",
       "   'train1',\n",
       "   'train1show',\n",
       "   'train1',\n",
       "   'transformed',\n",
       "   'train',\n",
       "   'try',\n",
       "   'train1',\n",
       "   'transform',\n",
       "   'train1test1',\n",
       "   'transform',\n",
       "   'train1test1',\n",
       "   'train1test1',\n",
       "   'train1',\n",
       "   't1transformtrain1',\n",
       "   'test1',\n",
       "   't1transformtest1',\n",
       "   'transformed',\n",
       "   'train1',\n",
       "   'test1',\n",
       "   'train1show',\n",
       "   'train1',\n",
       "   'test1',\n",
       "   'train1',\n",
       "   'test1',\n",
       "   'transformed',\n",
       "   'transom',\n",
       "   'train1',\n",
       "   'test1',\n",
       "   'train1selectfeaturesshow',\n",
       "   'train1selectlabelshow',\n",
       "   'transforming',\n",
       "   'task',\n",
       "   'train1',\n",
       "   'traincv',\n",
       "   'testcv',\n",
       "   'train1',\n",
       "   'traincv',\n",
       "   'testcv',\n",
       "   'traincv',\n",
       "   'testcv',\n",
       "   'train1randomsplit07',\n",
       "   'traincv',\n",
       "   'testcv',\n",
       "   'testcv',\n",
       "   'testcv',\n",
       "   'train1',\n",
       "   'things',\n",
       "   'tutorials',\n",
       "   'time'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, list(value)) for k, value in rdd5.take(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey / reduceByKey \n",
    "\n",
    "#### Q4. Frecuency of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd6 = rdd4.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd7 = rdd6.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', 2),\n",
       " ('moment', 1),\n",
       " ('1', 12),\n",
       " ('qunitillion', 1),\n",
       " ('million', 1),\n",
       " ('billion', 1),\n",
       " ('imagine', 4),\n",
       " ('drives', 1),\n",
       " ('cds', 1),\n",
       " ('blueray', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(key, len(list(value))) for key, value in rdd7.take(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd7_freq_of_words = rdd7.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(91, 'spark'),\n",
       " (74, 'data'),\n",
       " (52, 'apache'),\n",
       " (48, 'rdd'),\n",
       " (27, 'need'),\n",
       " (22, 'dataframe'),\n",
       " (22, 'cluster'),\n",
       " (22, 'train'),\n",
       " (21, 'lets'),\n",
       " (21, 'method')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7_freq_of_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(91, 'spark'),\n",
       " (74, 'data'),\n",
       " (52, 'apache'),\n",
       " (48, 'rdd'),\n",
       " (27, 'need'),\n",
       " (22, 'dataframe'),\n",
       " (22, 'cluster'),\n",
       " (22, 'train'),\n",
       " (21, 'lets'),\n",
       " (21, 'method')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions\n",
    "\n",
    "mapPartitions is a map transformation running in different partitions of the RDD. For instances, if we are curious about the frequency of 10 specific words, it's possible count them in different partitions.\n",
    "\n",
    "#### Q5. Perform a task in different partitions of the RDD: Investigate the frequency of the following words: \n",
    "- spark\n",
    "- apache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a function with the task required and then, pass the function to the **mapPartitions** transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def func(iterator):\n",
    "    \n",
    "    counter = Counter(iterator)\n",
    "    return (counter['spark'], counter['apache'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the number of partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we increase the number of partitions applying the function repartition over the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "repartrdd = rdd4.repartition(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of partitions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartrdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the function to the mapPartitions -using the original RDD- performs the task in just one partition. **glom** function allows us inspect the data into every partition. We use this tool to compare the original output and others generated changing the number of partitions. In the first case, as we could expect, the output is a single array with the frecuency of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[91, 52]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.mapPartitions(func).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for the new RDD with 6 partitions, we expect 6 outputs with two elements each one, corresponding to the frecuency of the pair of words searched in every partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 8], [15, 8], [15, 8], [20, 10], [16, 11], [13, 7]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartrdd.mapPartitions(func).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which function do we use to reduce the number of partitions? Let's inspect the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalescerdd = repartrdd.coalesce(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*coalesce* is the function to reduce the number of partitions. For this new RDD with 4 partitions, we expect 4 outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 8], [33, 17], [30, 16], [16, 11]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalescerdd.mapPartitions(func).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6: How do we get a sample of a population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sample* transformation has the following parameters:\n",
    "- withReplacement (bool)\n",
    "- fraction (float)\n",
    "- seed, random state (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_sample = rdd4.sample(False, 0.2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd_sample.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd4.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union/join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: Union of two RDD's. One with all the words starting with 'u' and another with the words ending with 'a'.\n",
    "#### Notes: union doesn't delete duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_words = rdd4.filter(lambda x: x[0] == 'e').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exciting',\n",
       " 'entirely',\n",
       " 'effectively',\n",
       " 'experience',\n",
       " 'efficient',\n",
       " 'emanating',\n",
       " 'efficiently',\n",
       " 'example',\n",
       " 'end',\n",
       " 'explain',\n",
       " 'exact',\n",
       " 'exactly',\n",
       " 'examples',\n",
       " 'enables',\n",
       " 'executors',\n",
       " 'ends',\n",
       " 'evaluation',\n",
       " 'execute',\n",
       " 'entire',\n",
       " 'easiest',\n",
       " 'extract',\n",
       " 'extracted',\n",
       " 'editing',\n",
       " 'environment',\n",
       " 'editor',\n",
       " 'export',\n",
       " 'extent',\n",
       " 'exploring',\n",
       " 'elements',\n",
       " 'element',\n",
       " 'existing',\n",
       " 'external',\n",
       " 'earlier',\n",
       " 'executing',\n",
       " 'elegant',\n",
       " 'encode',\n",
       " 'extra',\n",
       " 'evaluate',\n",
       " 'error',\n",
       " 'evaluator',\n",
       " 'evaluatorevaluatepredictionsevaluatormetricnamemse']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_a = rdd4.filter(lambda x: x[-1] == 'a').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'phenomena',\n",
       " 'scala',\n",
       " 'california',\n",
       " 'java',\n",
       " 'ppawebupd8teamjava',\n",
       " 'metadata',\n",
       " 'scparallelizedata',\n",
       " 'gupta',\n",
       " 'lambda',\n",
       " 'rddmaplambda',\n",
       " 'trueinferschema',\n",
       " 'inferschema',\n",
       " 'printschema',\n",
       " 'schema',\n",
       " 'trainprintschema',\n",
       " 'fillna',\n",
       " 'comma',\n",
       " 'formula',\n",
       " 'rformula',\n",
       " 'extra']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = e_words.union(words_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length words starting with \"e\": 41\n",
      "Length words finishing with \"a\": 21\n",
      "Lenght union: 62\n"
     ]
    }
   ],
   "source": [
    "print('Length words starting with \"e\": {}'.format(len(e_words.collect())))\n",
    "print('Length words finishing with \"a\": {}'.format(len(words_a.collect())))     \n",
    "print('Lenght union: {}'.format(len(union.collect())))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: Joining two pair RDD's based on their key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = rdd6.sample(False, 0.2, 42).distinct()\n",
    "sample2 = rdd6.sample(False, 0.2, 21).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', 1),\n",
       " ('qunitillion', 1),\n",
       " ('1', 1),\n",
       " ('blueray', 1),\n",
       " ('difficult', 1),\n",
       " ('data', 1),\n",
       " ('generation', 1),\n",
       " ('find', 1),\n",
       " ('handle', 1),\n",
       " ('huge', 1)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_samples = sample1.join(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', (1, 1)),\n",
       " ('qunitillion', (1, 1)),\n",
       " ('1', (1, 1)),\n",
       " ('huge', (1, 1)),\n",
       " ('offering', (1, 1)),\n",
       " ('based', (1, 1)),\n",
       " ('large', (1, 1)),\n",
       " ('read', (1, 1)),\n",
       " ('learning', (1, 1)),\n",
       " ('challenges', (1, 1))]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_samples.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length sample 1: 335\n",
      "Length sample 2: 356\n",
      "Lenght joined samples: 166\n"
     ]
    }
   ],
   "source": [
    "print('Length sample 1: {}'.format(len(sample1.collect())))\n",
    "print('Length sample 2: {}'.format(len(sample2.collect())))     \n",
    "print('Lenght joined samples: {}'.format(len(joined_samples.collect())))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_dataframe = SparkSession.builder.appName(\"pysparkDataframes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStorm2019 = sc_dataframe.read.format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .option('delimiter', ',')\\\n",
    "                .option('inferSchema', 'true')\\\n",
    "                .load('../pyspark/data/StormEvents_details-ftp_v1.0_d2019_c20200317.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----------+-------------+-------+--------+----------+--------+--------------+----------+----+----------+--------------------+-------+-------+--------------------+---+------------------+-----------+------------------+---------------+-----------------+-------------+---------------+---------------+------------+--------------------+---------+--------------+-----------+--------+-----------+----------+---------+-------------+------------------+-----------------+-----------------+-----------+-------------+----------------+---------+-----------+----------------+---------+---------+-------+--------+--------------------+--------------------+-----------+\n",
      "|BEGIN_YEARMONTH|BEGIN_DAY|BEGIN_TIME|END_YEARMONTH|END_DAY|END_TIME|EPISODE_ID|EVENT_ID|         STATE|STATE_FIPS|YEAR|MONTH_NAME|          EVENT_TYPE|CZ_TYPE|CZ_FIPS|             CZ_NAME|WFO|   BEGIN_DATE_TIME|CZ_TIMEZONE|     END_DATE_TIME|INJURIES_DIRECT|INJURIES_INDIRECT|DEATHS_DIRECT|DEATHS_INDIRECT|DAMAGE_PROPERTY|DAMAGE_CROPS|              SOURCE|MAGNITUDE|MAGNITUDE_TYPE|FLOOD_CAUSE|CATEGORY|TOR_F_SCALE|TOR_LENGTH|TOR_WIDTH|TOR_OTHER_WFO|TOR_OTHER_CZ_STATE|TOR_OTHER_CZ_FIPS|TOR_OTHER_CZ_NAME|BEGIN_RANGE|BEGIN_AZIMUTH|  BEGIN_LOCATION|END_RANGE|END_AZIMUTH|    END_LOCATION|BEGIN_LAT|BEGIN_LON|END_LAT| END_LON|   EPISODE_NARRATIVE|     EVENT_NARRATIVE|DATA_SOURCE|\n",
      "+---------------+---------+----------+-------------+-------+--------+----------+--------+--------------+----------+----+----------+--------------------+-------+-------+--------------------+---+------------------+-----------+------------------+---------------+-----------------+-------------+---------------+---------------+------------+--------------------+---------+--------------+-----------+--------+-----------+----------+---------+-------------+------------------+-----------------+-----------------+-----------+-------------+----------------+---------+-----------+----------------+---------+---------+-------+--------+--------------------+--------------------+-----------+\n",
      "|         201905|        9|      1554|       201905|      9|    1830|    137295|  824116|         TEXAS|        48|2019|       May|         Flash Flood|      C|     29|               BEXAR|EWX|09-MAY-19 15:54:00|      CST-6|09-MAY-19 18:30:00|              0|                0|            0|              0|          0.00K|       0.00K|     Law Enforcement|     null|          null| Heavy Rain|    null|       null|      null|     null|         null|              null|             null|             null|          8|            N|       LEON SPGS|        7|        NNE|    SAN GERONIMO|  29.7898| -98.6406|29.7158|-98.7744|Thunderstorms dev...|Thunderstorms pro...|        CSV|\n",
      "|         201907|       15|      1640|       201907|     15|    1641|    140217|  843354|     MINNESOTA|        27|2019|      July|   Thunderstorm Wind|      C|    115|                PINE|DLH|15-JUL-19 16:40:00|      CST-6|15-JUL-19 16:41:00|              0|                0|            0|              0|          0.00K|       0.00K|     Trained Spotter|     50.0|            EG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          5|            E|      ROCK CREEK|        5|          E|      ROCK CREEK|    45.77|   -92.87|  45.77|  -92.87|An area of low pr...|A 14-inch tree fe...|        CSV|\n",
      "|         201910|       20|      2223|       201910|     20|    2223|    142648|  861581|         TEXAS|        48|2019|   October|   Thunderstorm Wind|      C|    467|           VAN ZANDT|FWD|20-OCT-19 22:23:00|      CST-6|20-OCT-19 22:23:00|              0|                0|            0|              0|          0.00K|       0.00K|     Trained Spotter|     58.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          1|            N|        EDGEWOOD|        1|          N|        EDGEWOOD|    32.71|   -95.88|  32.71|  -95.88|Thunderstorms eru...|A trained spotter...|        CSV|\n",
      "|         201910|       20|      2312|       201910|     20|    2312|    142648|  861584|         TEXAS|        48|2019|   October|   Thunderstorm Wind|      C|    439|             TARRANT|FWD|20-OCT-19 23:12:00|      CST-6|20-OCT-19 23:12:00|              0|                0|            0|              0|          0.00K|       0.00K|     Law Enforcement|     56.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          5|          WSW|            AZLE|        5|        WSW|            AZLE|    32.87|   -97.61|  32.87|  -97.61|Thunderstorms eru...|The local police ...|        CSV|\n",
      "|         201910|       20|      2236|       201910|     20|    2236|    142648|  861582|         TEXAS|        48|2019|   October|   Thunderstorm Wind|      C|    363|          PALO PINTO|FWD|20-OCT-19 22:36:00|      CST-6|20-OCT-19 22:36:00|              0|                0|            0|              0|          0.00K|       0.00K|                ASOS|     65.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          0|            N|   MINERAL WELLS|        0|          N|   MINERAL WELLS|     32.8|    -98.1|   32.8|   -98.1|Thunderstorms eru...|A wind gust of 75...|        CSV|\n",
      "|         201910|       20|      2048|       201910|     20|    2054|    142648|  856504|         TEXAS|        48|2019|   October|             Tornado|      C|    397|            ROCKWALL|FWD|20-OCT-19 20:48:00|      CST-6|20-OCT-19 20:54:00|              0|                0|            0|              0|         10.00M|       0.00K|    NWS Storm Survey|     null|          null|       null|    null|        EF1|      1.96|    150.0|         null|              null|             null|             null|          2|            W|        ROCKWALL|        1|        NNW|        ROCKWALL|  32.9312| -96.4864|32.9393|-96.4545|Thunderstorms eru...|Tornadic wind dam...|        CSV|\n",
      "|         201909|        4|      1229|       201909|      4|    1229|    141212|  848333|       VERMONT|        50|2019| September|                Hail|      C|     27|             WINDSOR|BTV|04-SEP-19 12:29:00|      EST-5|04-SEP-19 12:29:00|              0|                0|            0|              0|          0.00K|       0.00K|              Public|      1.0|          null|       null|    null|       null|      null|     null|         null|              null|             null|             null|          1|            W|          LUDLOW|        1|          W|          LUDLOW|     43.4|   -72.72|   43.4|  -72.72|A strong mid-leve...|Quarter size hail...|        CSV|\n",
      "|         201909|       26|      1554|       201909|     26|    1554|    141215|  848338|      NEW YORK|        36|2019| September|   Thunderstorm Wind|      C|     89|        ST. LAWRENCE|BTV|26-SEP-19 15:54:00|      EST-5|26-SEP-19 15:54:00|              0|                0|            0|              0|          5.00K|       0.00K|     Utility Company|     50.0|            EG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          5|           SW|          MADRID|        5|         SW|          MADRID|  44.6988|  -75.222|44.6988| -75.222|A strong mid-leve...|A few trees and p...|        CSV|\n",
      "|         201909|       13|      1838|       201909|     13|    1838|    140688|  845760|ATLANTIC SOUTH|        87|2019| September|Marine Thundersto...|      Z|    552|VOLUSIA-BREVARD C...|MLB|13-SEP-19 18:38:00|      EST-5|13-SEP-19 18:38:00|              0|                0|            0|              0|          0.00K|       0.00K|             Mesonet|     36.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          1|            S|  CAPE CANAVERAL|        1|          S|  CAPE CANAVERAL|    28.37|    -80.6|  28.37|   -80.6|Fast moving showe...|Weatherflow site ...|        CSV|\n",
      "|         201909|       13|      1915|       201909|     13|    1915|    140688|  845762|ATLANTIC SOUTH|        87|2019| September|Marine Thundersto...|      Z|    552|VOLUSIA-BREVARD C...|MLB|13-SEP-19 19:15:00|      EST-5|13-SEP-19 19:15:00|              0|                0|            0|              0|          0.00K|       0.00K|Other Federal Agency|     36.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          9|           NE|PLAYALINDA BEACH|        9|         NE|PLAYALINDA BEACH|    28.63|   -80.66|  28.63|  -80.66|Fast moving showe...|US Air Force wind...|        CSV|\n",
      "+---------------+---------+----------+-------------+-------+--------+----------+--------+--------------+----------+----+----------+--------------------+-------+-------+--------------------+---+------------------+-----------+------------------+---------------+-----------------+-------------+---------------+---------------+------------+--------------------+---------+--------------+-----------+--------+-----------+----------+---------+-------------+------------------+-----------------+-----------------+-----------+-------------+----------------+---------+-----------+----------------+---------+---------+-------+--------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67337"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStorm2019.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking duplicates rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfStorm2019.count() == dfStorm2019.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BEGIN_YEARMONTH: integer (nullable = true)\n",
      " |-- BEGIN_DAY: integer (nullable = true)\n",
      " |-- BEGIN_TIME: integer (nullable = true)\n",
      " |-- END_YEARMONTH: integer (nullable = true)\n",
      " |-- END_DAY: integer (nullable = true)\n",
      " |-- END_TIME: integer (nullable = true)\n",
      " |-- EPISODE_ID: integer (nullable = true)\n",
      " |-- EVENT_ID: integer (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- STATE_FIPS: integer (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH_NAME: string (nullable = true)\n",
      " |-- EVENT_TYPE: string (nullable = true)\n",
      " |-- CZ_TYPE: string (nullable = true)\n",
      " |-- CZ_FIPS: integer (nullable = true)\n",
      " |-- CZ_NAME: string (nullable = true)\n",
      " |-- WFO: string (nullable = true)\n",
      " |-- BEGIN_DATE_TIME: string (nullable = true)\n",
      " |-- CZ_TIMEZONE: string (nullable = true)\n",
      " |-- END_DATE_TIME: string (nullable = true)\n",
      " |-- INJURIES_DIRECT: integer (nullable = true)\n",
      " |-- INJURIES_INDIRECT: integer (nullable = true)\n",
      " |-- DEATHS_DIRECT: integer (nullable = true)\n",
      " |-- DEATHS_INDIRECT: integer (nullable = true)\n",
      " |-- DAMAGE_PROPERTY: string (nullable = true)\n",
      " |-- DAMAGE_CROPS: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- MAGNITUDE: double (nullable = true)\n",
      " |-- MAGNITUDE_TYPE: string (nullable = true)\n",
      " |-- FLOOD_CAUSE: string (nullable = true)\n",
      " |-- CATEGORY: integer (nullable = true)\n",
      " |-- TOR_F_SCALE: string (nullable = true)\n",
      " |-- TOR_LENGTH: double (nullable = true)\n",
      " |-- TOR_WIDTH: double (nullable = true)\n",
      " |-- TOR_OTHER_WFO: string (nullable = true)\n",
      " |-- TOR_OTHER_CZ_STATE: string (nullable = true)\n",
      " |-- TOR_OTHER_CZ_FIPS: integer (nullable = true)\n",
      " |-- TOR_OTHER_CZ_NAME: string (nullable = true)\n",
      " |-- BEGIN_RANGE: integer (nullable = true)\n",
      " |-- BEGIN_AZIMUTH: string (nullable = true)\n",
      " |-- BEGIN_LOCATION: string (nullable = true)\n",
      " |-- END_RANGE: integer (nullable = true)\n",
      " |-- END_AZIMUTH: string (nullable = true)\n",
      " |-- END_LOCATION: string (nullable = true)\n",
      " |-- BEGIN_LAT: double (nullable = true)\n",
      " |-- BEGIN_LON: double (nullable = true)\n",
      " |-- END_LAT: double (nullable = true)\n",
      " |-- END_LON: double (nullable = true)\n",
      " |-- EPISODE_NARRATIVE: string (nullable = true)\n",
      " |-- EVENT_NARRATIVE: string (nullable = true)\n",
      " |-- DATA_SOURCE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----------+-------------+-------+--------+----------+--------+-----+----------+----+----------+-----------------+-------+-------+----------+---+------------------+-----------+------------------+---------------+-----------------+-------------+---------------+---------------+------------+--------------------+---------+--------------+-----------+--------+-----------+----------+---------+-------------+------------------+-----------------+-----------------+-----------+-------------+--------------+---------+-----------+-------------+---------+---------+-------+--------+--------------------+--------------------+-----------+\n",
      "|BEGIN_YEARMONTH|BEGIN_DAY|BEGIN_TIME|END_YEARMONTH|END_DAY|END_TIME|EPISODE_ID|EVENT_ID|STATE|STATE_FIPS|YEAR|MONTH_NAME|       EVENT_TYPE|CZ_TYPE|CZ_FIPS|   CZ_NAME|WFO|   BEGIN_DATE_TIME|CZ_TIMEZONE|     END_DATE_TIME|INJURIES_DIRECT|INJURIES_INDIRECT|DEATHS_DIRECT|DEATHS_INDIRECT|DAMAGE_PROPERTY|DAMAGE_CROPS|              SOURCE|MAGNITUDE|MAGNITUDE_TYPE|FLOOD_CAUSE|CATEGORY|TOR_F_SCALE|TOR_LENGTH|TOR_WIDTH|TOR_OTHER_WFO|TOR_OTHER_CZ_STATE|TOR_OTHER_CZ_FIPS|TOR_OTHER_CZ_NAME|BEGIN_RANGE|BEGIN_AZIMUTH|BEGIN_LOCATION|END_RANGE|END_AZIMUTH| END_LOCATION|BEGIN_LAT|BEGIN_LON|END_LAT| END_LON|   EPISODE_NARRATIVE|     EVENT_NARRATIVE|DATA_SOURCE|\n",
      "+---------------+---------+----------+-------------+-------+--------+----------+--------+-----+----------+----+----------+-----------------+-------+-------+----------+---+------------------+-----------+------------------+---------------+-----------------+-------------+---------------+---------------+------------+--------------------+---------+--------------+-----------+--------+-----------+----------+---------+-------------+------------------+-----------------+-----------------+-----------+-------------+--------------+---------+-----------+-------------+---------+---------+-------+--------+--------------------+--------------------+-----------+\n",
      "|         201905|        9|      1554|       201905|      9|    1830|    137295|  824116|TEXAS|        48|2019|       May|      Flash Flood|      C|     29|     BEXAR|EWX|09-MAY-19 15:54:00|      CST-6|09-MAY-19 18:30:00|              0|                0|            0|              0|          0.00K|       0.00K|     Law Enforcement|     null|          null| Heavy Rain|    null|       null|      null|     null|         null|              null|             null|             null|          8|            N|     LEON SPGS|        7|        NNE| SAN GERONIMO|  29.7898| -98.6406|29.7158|-98.7744|Thunderstorms dev...|Thunderstorms pro...|        CSV|\n",
      "|         201910|       20|      2223|       201910|     20|    2223|    142648|  861581|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    467| VAN ZANDT|FWD|20-OCT-19 22:23:00|      CST-6|20-OCT-19 22:23:00|              0|                0|            0|              0|          0.00K|       0.00K|     Trained Spotter|     58.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          1|            N|      EDGEWOOD|        1|          N|     EDGEWOOD|    32.71|   -95.88|  32.71|  -95.88|Thunderstorms eru...|A trained spotter...|        CSV|\n",
      "|         201910|       20|      2312|       201910|     20|    2312|    142648|  861584|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    439|   TARRANT|FWD|20-OCT-19 23:12:00|      CST-6|20-OCT-19 23:12:00|              0|                0|            0|              0|          0.00K|       0.00K|     Law Enforcement|     56.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          5|          WSW|          AZLE|        5|        WSW|         AZLE|    32.87|   -97.61|  32.87|  -97.61|Thunderstorms eru...|The local police ...|        CSV|\n",
      "|         201910|       20|      2236|       201910|     20|    2236|    142648|  861582|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    363|PALO PINTO|FWD|20-OCT-19 22:36:00|      CST-6|20-OCT-19 22:36:00|              0|                0|            0|              0|          0.00K|       0.00K|                ASOS|     65.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          0|            N| MINERAL WELLS|        0|          N|MINERAL WELLS|     32.8|    -98.1|   32.8|   -98.1|Thunderstorms eru...|A wind gust of 75...|        CSV|\n",
      "|         201910|       20|      2048|       201910|     20|    2054|    142648|  856504|TEXAS|        48|2019|   October|          Tornado|      C|    397|  ROCKWALL|FWD|20-OCT-19 20:48:00|      CST-6|20-OCT-19 20:54:00|              0|                0|            0|              0|         10.00M|       0.00K|    NWS Storm Survey|     null|          null|       null|    null|        EF1|      1.96|    150.0|         null|              null|             null|             null|          2|            W|      ROCKWALL|        1|        NNW|     ROCKWALL|  32.9312| -96.4864|32.9393|-96.4545|Thunderstorms eru...|Tornadic wind dam...|        CSV|\n",
      "|         201905|       23|      2230|       201905|     23|    2230|    137864|  828397|TEXAS|        48|2019|       May|             Hail|      C|    211|  HEMPHILL|AMA|23-MAY-19 22:30:00|      CST-6|23-MAY-19 22:30:00|              0|                0|            0|              0|          0.00K|       0.00K|     Trained Spotter|      1.0|          null|       null|    null|       null|      null|     null|         null|              null|             null|             null|          2|          WNW|       GLAZIER|        2|        WNW|      GLAZIER|    36.01|  -100.26|  36.01| -100.26|A vigorous upper ...|                null|        CSV|\n",
      "|         201910|       21|       130|       201910|     21|     130|    142648|  861579|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    349|   NAVARRO|FWD|21-OCT-19 01:30:00|      CST-6|21-OCT-19 01:30:00|              0|                0|            0|              0|         10.00K|       0.00K|              Public|     55.0|            EG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          0|            N|        KERENS|        0|          N|       KERENS|    32.13|   -96.23|  32.13|  -96.23|Thunderstorms eru...|A public report i...|        CSV|\n",
      "|         201910|       20|      2336|       201910|     20|    2336|    142648|  861587|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    439|   TARRANT|FWD|20-OCT-19 23:36:00|      CST-6|20-OCT-19 23:36:00|              0|                0|            0|              0|          0.00K|       0.00K|                ASOS|     54.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          1|            E|    RIVER OAKS|        1|          E|   RIVER OAKS|    32.78|   -97.38|  32.78|  -97.38|Thunderstorms eru...|A 62 MPH wind gus...|        CSV|\n",
      "|         201910|       20|      2338|       201910|     20|    2338|    142648|  861588|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    439|   TARRANT|FWD|20-OCT-19 23:38:00|      CST-6|20-OCT-19 23:38:00|              0|                0|            0|              0|          0.00K|       0.00K|Official NWS Obse...|     55.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          0|            N|   HALTOM CITY|        0|          N|  HALTOM CITY|     32.8|   -97.27|   32.8|  -97.27|Thunderstorms eru...|A 63 MPH wind gus...|        CSV|\n",
      "|         201910|       20|      2340|       201910|     20|    2340|    142648|  861589|TEXAS|        48|2019|   October|Thunderstorm Wind|      C|    439|   TARRANT|FWD|20-OCT-19 23:40:00|      CST-6|20-OCT-19 23:40:00|              0|                0|            0|              0|          0.00K|       0.00K|                ASOS|     54.0|            MG|       null|    null|       null|      null|     null|         null|              null|             null|             null|          1|           SE|         HODGE|        1|         SE|        HODGE|    32.79|   -97.34|  32.79|  -97.34|Thunderstorms eru...|A 62 MPH wind gus...|        CSV|\n",
      "+---------------+---------+----------+-------------+-------+--------+----------+--------+-----+----------+----+----------+-----------------+-------+-------+----------+---+------------------+-----------+------------------+---------------+-----------------+-------------+---------------+---------------+------------+--------------------+---------+--------------+-----------+--------+-----------+----------+---------+-------------+------------------+-----------------+-----------------+-----------+-------------+--------------+---------+-----------+-------------+---------+---------+-------+--------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.filter(dfStorm2019.STATE == 'TEXAS').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfStorm2019.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               STATE|count|\n",
      "+--------------------+-----+\n",
      "|               TEXAS| 4338|\n",
      "|           MINNESOTA| 2126|\n",
      "|             VERMONT|  317|\n",
      "|            NEW YORK| 2514|\n",
      "|      ATLANTIC SOUTH|  519|\n",
      "|             FLORIDA|  939|\n",
      "|       WEST VIRGINIA|  967|\n",
      "|            ARKANSAS| 1148|\n",
      "|      GULF OF MEXICO|  907|\n",
      "|             MONTANA| 1286|\n",
      "|            MISSOURI| 2159|\n",
      "|             GEORGIA| 1158|\n",
      "|         CONNECTICUT|  240|\n",
      "|             ALABAMA| 1116|\n",
      "|                OHIO| 2279|\n",
      "|       MASSACHUSETTS|  860|\n",
      "|        RHODE ISLAND|   92|\n",
      "|              KANSAS| 2672|\n",
      "|                IOWA| 2276|\n",
      "|            VIRGINIA| 2398|\n",
      "|              ALASKA|  143|\n",
      "|      ATLANTIC NORTH| 1054|\n",
      "|            MARYLAND| 1208|\n",
      "|          CALIFORNIA| 2643|\n",
      "|        SOUTH DAKOTA| 2543|\n",
      "|      NORTH CAROLINA| 1448|\n",
      "|         MISSISSIPPI| 1140|\n",
      "|DISTRICT OF COLUMBIA|   52|\n",
      "|        NORTH DAKOTA| 1095|\n",
      "|            COLORADO| 1776|\n",
      "|            KENTUCKY| 1522|\n",
      "|              NEVADA|  313|\n",
      "|           TENNESSEE| 1157|\n",
      "|           WISCONSIN| 1573|\n",
      "|               MAINE|  320|\n",
      "|            NEBRASKA| 2088|\n",
      "|             WYOMING| 1106|\n",
      "|             INDIANA| 1447|\n",
      "|            ILLINOIS| 2084|\n",
      "|               IDAHO|  585|\n",
      "|          NEW JERSEY|  901|\n",
      "|              HAWAII|  687|\n",
      "|          NEW MEXICO|  860|\n",
      "|       LAKE MICHIGAN|  105|\n",
      "|       LAKE SUPERIOR|  102|\n",
      "|          LAKE HURON|   21|\n",
      "|            OKLAHOMA| 1801|\n",
      "|                UTAH|  335|\n",
      "|            MICHIGAN| 1067|\n",
      "|           LOUISIANA|  813|\n",
      "|        PENNSYLVANIA| 2395|\n",
      "|            DELAWARE|  136|\n",
      "|          WASHINGTON|  414|\n",
      "|      SOUTH CAROLINA|  826|\n",
      "|             ARIZONA|  516|\n",
      "|              OREGON|  290|\n",
      "|         PUERTO RICO|  155|\n",
      "|           LAKE ERIE|   77|\n",
      "|       NEW HAMPSHIRE|  129|\n",
      "|      AMERICAN SAMOA|   21|\n",
      "+--------------------+-----+\n",
      "only showing top 60 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.groupBy('STATE').count().show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          EVENT_TYPE|count|\n",
      "+--------------------+-----+\n",
      "|         Flash Flood| 4068|\n",
      "|   Thunderstorm Wind|18617|\n",
      "|             Tornado| 1727|\n",
      "|                Hail| 9013|\n",
      "|Marine Thundersto...| 2502|\n",
      "|         Rip Current|   72|\n",
      "|      Winter Weather| 3800|\n",
      "|          Waterspout|  183|\n",
      "|          Heavy Rain| 1416|\n",
      "|             Drought| 1007|\n",
      "|        Winter Storm| 3312|\n",
      "|               Flood| 4943|\n",
      "|           Lightning|  343|\n",
      "|            Blizzard|  852|\n",
      "|Extreme Cold/Wind...| 1065|\n",
      "|          Heavy Snow| 2844|\n",
      "|           High Wind| 3743|\n",
      "|        Funnel Cloud|  348|\n",
      "|       Coastal Flood|  240|\n",
      "|      Excessive Heat|  827|\n",
      "|                Heat| 1291|\n",
      "|         Marine Hail|   32|\n",
      "|     Cold/Wind Chill|  470|\n",
      "|         Strong Wind| 1590|\n",
      "|         Debris Flow|  184|\n",
      "|           Dense Fog|  652|\n",
      "|               Sleet|    1|\n",
      "|        Frost/Freeze|  654|\n",
      "|           Avalanche|   44|\n",
      "|           Hurricane|   10|\n",
      "|      Tropical Storm|  143|\n",
      "|           High Surf|  468|\n",
      "|            Wildfire|  180|\n",
      "|    Marine High Wind|   77|\n",
      "|           Ice Storm|  200|\n",
      "|Astronomical Low ...|   16|\n",
      "|          Dust Devil|    8|\n",
      "|    Lake-Effect Snow|   78|\n",
      "|        Freezing Fog|    6|\n",
      "|     Lakeshore Flood|  146|\n",
      "|          Dust Storm|   40|\n",
      "|         Sneakerwave|    3|\n",
      "|  Marine Strong Wind|   17|\n",
      "|Marine Tropical S...|   37|\n",
      "|         Dense Smoke|    2|\n",
      "|    Storm Surge/Tide|   25|\n",
      "| Tropical Depression|   26|\n",
      "|Marine Tropical D...|    4|\n",
      "|              Seiche|    4|\n",
      "|Marine Hurricane/...|    6|\n",
      "+--------------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.groupBy('EVENT_TYPE').count().show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+\n",
      "|         STATE|MONTH_NAME|count|\n",
      "+--------------+----------+-----+\n",
      "|       ALABAMA|  February|  104|\n",
      "|       ALABAMA|   October|   35|\n",
      "|       ALABAMA| September|   41|\n",
      "|       ALABAMA|       May|   73|\n",
      "|       ALABAMA|      June|  168|\n",
      "|       ALABAMA|   January|   32|\n",
      "|       ALABAMA|      July|   63|\n",
      "|       ALABAMA|    August|  126|\n",
      "|       ALABAMA|     March|  148|\n",
      "|       ALABAMA|  December|   92|\n",
      "|       ALABAMA|  November|   77|\n",
      "|       ALABAMA|     April|  157|\n",
      "|        ALASKA|     April|   12|\n",
      "|        ALASKA|   October|   19|\n",
      "|        ALASKA|      June|    4|\n",
      "|        ALASKA|     March|   17|\n",
      "|        ALASKA|  February|   14|\n",
      "|        ALASKA|   January|   10|\n",
      "|        ALASKA|      July|    2|\n",
      "|        ALASKA|       May|    2|\n",
      "|        ALASKA|    August|   11|\n",
      "|        ALASKA| September|    4|\n",
      "|        ALASKA|  December|   32|\n",
      "|        ALASKA|  November|   16|\n",
      "|AMERICAN SAMOA|       May|    2|\n",
      "|AMERICAN SAMOA| September|    3|\n",
      "|AMERICAN SAMOA|      July|    2|\n",
      "|AMERICAN SAMOA|   October|    2|\n",
      "|AMERICAN SAMOA|  December|    4|\n",
      "|AMERICAN SAMOA|      June|    4|\n",
      "|AMERICAN SAMOA|     March|    1|\n",
      "|AMERICAN SAMOA|    August|    2|\n",
      "|AMERICAN SAMOA|     April|    1|\n",
      "|       ARIZONA|     March|    9|\n",
      "|       ARIZONA|   January|   32|\n",
      "|       ARIZONA|     April|    6|\n",
      "|       ARIZONA|    August|   82|\n",
      "|       ARIZONA|  February|   72|\n",
      "|       ARIZONA|      June|    7|\n",
      "|       ARIZONA| September|  122|\n",
      "|       ARIZONA|  November|   87|\n",
      "|       ARIZONA|       May|    6|\n",
      "|       ARIZONA|      July|   64|\n",
      "|       ARIZONA|   October|    3|\n",
      "|       ARIZONA|  December|   26|\n",
      "|      ARKANSAS|     March|   70|\n",
      "|      ARKANSAS|     April|   89|\n",
      "|      ARKANSAS|   January|   46|\n",
      "|      ARKANSAS|       May|  220|\n",
      "|      ARKANSAS|  February|   60|\n",
      "+--------------+----------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.groupBy('STATE', 'MONTH_NAME').count().orderBy('STATE', ascending=True).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|sum(DEATHS_DIRECT)|\n",
      "+------------------+\n",
      "|               390|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.agg({'DEATHS_DIRECT':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DEATHS_INDIRECT)|\n",
      "+--------------------+\n",
      "|                 160|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStorm2019.agg({'DEATHS_INDIRECT':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
